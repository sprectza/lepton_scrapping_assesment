{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc3fcb9f",
   "metadata": {},
   "source": [
    "##### Documentation \n",
    "\n",
    "Greetings for the day, \n",
    "\n",
    "Thank you for considering my application and assigning this task, as it was asked I have provided the approach and challenges I faced during this assignment. \n",
    "\n",
    "The task was to scrap data from our favourite retail website, a few website that came to mind were, VMart, Reliance Fresh, Max, etc. After inspecting all of their codes and understanding the underlying HTML structure I went ahead with scrapping data of all the VMart stores located in Karnataka (I am currently living in Karnataka). \n",
    "\n",
    "The approach: \n",
    "Since the website is dynamic and loads content through JavaScript I decided to use Selenium as my choice of library to scrap the data needed by following the XPaths that are provided when we inspect the code of the website. The idea was to iterate over divisions where the relevant information was present and extract all the data based on the XPath values.\n",
    "\n",
    "Challenge 1: Iterating over multiple cards\n",
    "Initially I only wrote the code in a way that it was only able to extract details from a single card but we needed to get details from all the cards that are available, to do that I wrote a simple for loop after analysing the values in XPaths that are going to be changed as we iterate through cards. Finally I was able to extract the relevant information from all the cards.\n",
    "\n",
    "Challenge 2: Extracting the coordinates from the cards \n",
    "I was trying to figure out how to tackle this problem and the first idea that was in my mind was to just use a Regex to extract the particular numeric value, but the issue was where to find this numeric value, after analysing the Location button (the button which opened up google maps in a new link) I decided to work on the link that is given for each location, every maps link at the end has exact coordinates of the location for example, https://www.google.com/maps/dir/?api=1&origin=&destination=15.152952,76.933163 in this case the coordinates are going to be \"15.152952,76.933163\". First I used the get_attribute(\"href\") method provided by selenium to find the actual value and then split the URL using \"destination=\" as delimiter to get the coordinates. \n",
    "\n",
    "I also faced some issues with the chromedriver, but sorted it out after finding the correct version supported by Google for their Chromium browser. \n",
    "\n",
    "Finally I used pandas to manage the data that is extracted and push it to the a CSV file as it was required. \n",
    "\n",
    "Learnings: I had previously scrapped websites but most of them were static and actually never used selenium as a scrapping tool, it was a fun learning experience to understand how to use these tools to get the data that is needed. \n",
    "\n",
    "Sincerely, \n",
    "Swarit Pandey. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e282fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9662798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3281002/525287513.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=\"/home/sprectza/internship_tasks/lepton_tasks/chromedriver_linux/chromedriver\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped and saved to vmart_stores_coordinates.csv\n"
     ]
    }
   ],
   "source": [
    "# Setting up the webdriver\n",
    "driver = webdriver.Chrome(executable_path=\"/home/sprectza/internship_tasks/lepton_tasks/chromedriver_linux/chromedriver\")\n",
    "\n",
    "url = \"https://stores.vmartretail.com/location/karnataka\"\n",
    "driver.get(url)\n",
    "\n",
    "# Finding the total number of cards\n",
    "total_cards = len(driver.find_elements(By.XPATH, '//*[@id=\"main-view-scroll\"]/div/div/div/div[2]/div'))\n",
    "\n",
    "# Extracting data for each card\n",
    "data = []\n",
    "for i in range(1, total_cards + 1):\n",
    "    address_xpath = f'//*[@id=\"main-view-scroll\"]/div/div/div/div[2]/div[{i}]/div/div[2]/ul/li/span[2]'\n",
    "    city_xpath = f'//*[@id=\"main-view-scroll\"]/div/div/div/div[2]/div[{i}]/div/div[2]/div/ul/li[1]/span[2]'\n",
    "    phone_xpath = f'//*[@id=\"main-view-scroll\"]/div/div/div/div[2]/div[{i}]/div/div[2]/div/ul/li[2]/span[2]/a'\n",
    "    timing_xpath = f'//*[@id=\"main-view-scroll\"]/div/div/div/div[2]/div[{i}]/div/div[2]/div/ul/li[3]/span[2]'\n",
    "    location_xpath = f'//*[@id=\"main-view-scroll\"]/div/div/div/div[2]/div[{i}]/div/div[3]/div/div/div[3]/a'\n",
    "\n",
    "    address = driver.find_element(By.XPATH, address_xpath).text\n",
    "    city = driver.find_element(By.XPATH, city_xpath).text\n",
    "    phone = driver.find_element(By.XPATH, phone_xpath).text\n",
    "    timing = driver.find_element(By.XPATH, timing_xpath).text\n",
    "    location_url = driver.find_element(By.XPATH, location_xpath).get_attribute(\"href\")\n",
    "\n",
    "    # Extraction coordinates from the URL\n",
    "    coordinates = location_url.split(\"destination=\")[-1]\n",
    "\n",
    "    data.append({\n",
    "        'Address': address,\n",
    "        'City': city,\n",
    "        'Phone Number': phone,\n",
    "        'Timings': timing,\n",
    "        'Coordinates': coordinates\n",
    "    })\n",
    "\n",
    "# Save the data to a CSV file and quit the driver\n",
    "data_frame = pd.DataFrame(data)\n",
    "data_frame.to_csv(\"vmart_stores_info.csv\", index=False)\n",
    "driver.quit()\n",
    "\n",
    "print(\"Data scraped and saved to vmart_stores_info.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
